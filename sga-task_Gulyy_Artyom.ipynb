{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User routes on the site\n",
    "## Description\n",
    "**Clickstream** is a sequence of user actions on a website. It allows you to understand how users interact with the site. In this task, you need to find the most frequent custom routes.\n",
    "\n",
    "## Input data\n",
    "Input data is а table with clickstream data in file `hdfs:/data/clickstream.csv`.\n",
    "\n",
    "### Table structure\n",
    "* `user_id (int)` - Unique user identifier.\n",
    "* `session_id (int)` - Unique identifier for the user session. The user's session lasts until the identifier changes.\n",
    "* `event_type (string)` - Event type from the list:\n",
    "    * **page** - visit to the page\n",
    "    * **event** - any action on the page\n",
    "    * <b>&lt;custom&gt;</b> - string with any other type\n",
    "* `event_type (string)` - Page on the site.\n",
    "* `timestamp (int)` - Unix-timestamp of action.\n",
    "\n",
    "### Browser errors\n",
    "Errors can sometimes occur in the user's browser - after such an error appears, we can no longer trust the data of this session and all the following lines after the error or at the same time with it are considered corrupted and **should not be counted** in statistics.\n",
    "\n",
    "When an error occurs on the page, a random string containing the word **error** will be written to the `event_type` field.\n",
    "\n",
    "### Sample of user session\n",
    "<pre>\n",
    "+-------+----------+------------+----------+----------+\n",
    "|user_id|session_id|  event_type|event_page| timestamp|\n",
    "+-------+----------+------------+----------+----------+\n",
    "|    562|       507|        page|      main|1620494781|\n",
    "|    562|       507|       event|      main|1620494788|\n",
    "|    562|       507|       event|      main|1620494798|\n",
    "|    562|       507|        page|    family|1620494820|\n",
    "|    562|       507|       event|    family|1620494828|\n",
    "|    562|       507|        page|      main|1620494848|\n",
    "|    562|       507|wNaxLlerrorU|      main|1620494865|\n",
    "|    562|       507|       event|      main|1620494873|\n",
    "|    562|       507|        page|      news|1620494875|\n",
    "|    562|       507|        page|   tariffs|1620494876|\n",
    "|    562|       507|       event|   tariffs|1620494884|\n",
    "|    562|       514|        page|      main|1620728918|\n",
    "|    562|       514|       event|      main|1620729174|\n",
    "|    562|       514|        page|   archive|1620729674|\n",
    "|    562|       514|        page|     bonus|1620729797|\n",
    "|    562|       514|        page|   tariffs|1620731090|\n",
    "|    562|       514|       event|   tariffs|1620731187|\n",
    "+-------+----------+------------+----------+----------+\n",
    "</pre>\n",
    "\n",
    "#### Correct user routes for a given user:\n",
    "* **Session 507**: main-family-main\n",
    "* **Session 514**: main-archive-bonus-tariffs\n",
    "\n",
    "Route elements are ordered by the time they appear in the clickstream, from earliest to latest.\n",
    "\n",
    "The route must be accounted for completely before the end of the session or an error in the session.\n",
    "\n",
    "## Task\n",
    "You need to use the Spark SQL, Spark RDD and Spark DF interfaces to create a solution file, the lines of which contain **the 30 most frequent user routes** on the site.\n",
    "\n",
    "Each line of the file should contain the `route` and `count` values **separated by tabs**, where:\n",
    "* `route` - route on the site, consisting of pages separated by \"-\".\n",
    "* `count` - the number of user sessions in which this route was.\n",
    "\n",
    "The lines must be **ordered in descending order** of the `count` field.\n",
    "\n",
    "## Criteria\n",
    "You can get maximum of 3.5 points (final grade) for this assignment, depedning on the number of interface you manage to leverage. The criteria are as follows:\n",
    "\n",
    "* 0.5 points – Spark SQL solution with 1 query\n",
    "* 0.5 points – Spark SQL solution with <=2 queries\n",
    "* 0.5 points – Spark RDD solution\n",
    "* 0.5 points – Spark DF solution\n",
    "* 0.5 points – your solution algorithm is relatively optimized, i.e.: no O^2 or O^3 complexities; appropriate object usage; no data leaks etc. This is evaluated by staff.\n",
    "* 1 point – 1 on 1 screening session. During this session staff member can ask you questions regarding your solution logic, framework usage, questionable parts of your code etc. If your code is clean enough, the staff member can just ask you to solve a theoretical problem connected to Spark.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"UserRoutesSQL\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read the data from HDFS\n",
    "df = spark.read.csv(\n",
    "    \"hdfs:/data/clickstream.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    sep='\\t',\n",
    "    quote='\"',\n",
    "    escape='\"'\n",
    ")\n",
    "\n",
    "\n",
    "df.createOrReplaceTempView('clickstream')\n",
    "\n",
    "# Execute the SQL query with LIMIT 30\n",
    "result = spark.sql(\"\"\"\n",
    "WITH error_events AS (\n",
    "    SELECT\n",
    "        user_id,\n",
    "        session_id,\n",
    "        MIN(CASE WHEN LOWER(event_type) LIKE '%error%' THEN timestamp END) AS min_error_timestamp\n",
    "    FROM clickstream\n",
    "    GROUP BY user_id, session_id\n",
    "),\n",
    "filtered_events AS (\n",
    "    SELECT\n",
    "        c.user_id,\n",
    "        c.session_id,\n",
    "        c.event_type,\n",
    "        c.event_page,\n",
    "        c.timestamp,\n",
    "        e.min_error_timestamp\n",
    "    FROM clickstream c\n",
    "    LEFT JOIN error_events e\n",
    "        ON c.user_id = e.user_id AND c.session_id = e.session_id\n",
    "    WHERE e.min_error_timestamp IS NULL OR c.timestamp <= e.min_error_timestamp\n",
    "),\n",
    "page_events AS (\n",
    "    SELECT\n",
    "        user_id,\n",
    "        session_id,\n",
    "        event_page,\n",
    "        timestamp,\n",
    "        ROW_NUMBER() OVER (PARTITION BY user_id, session_id ORDER BY timestamp) AS rn,\n",
    "        LAG(event_page) OVER (PARTITION BY user_id, session_id ORDER BY timestamp) AS prev_event_page\n",
    "    FROM filtered_events\n",
    "    WHERE event_type = 'page'\n",
    "),\n",
    "distinct_page_events AS (\n",
    "    SELECT\n",
    "        user_id,\n",
    "        session_id,\n",
    "        event_page,\n",
    "        rn\n",
    "    FROM page_events\n",
    "    WHERE event_page != prev_event_page OR prev_event_page IS NULL\n",
    "),\n",
    "routes AS (\n",
    "    SELECT\n",
    "        user_id,\n",
    "        session_id,\n",
    "        CONCAT_WS('-', TRANSFORM(\n",
    "            ARRAY_SORT(COLLECT_LIST(NAMED_STRUCT('rn', rn, 'event_page', event_page))),\n",
    "            x -> x.event_page)) AS route\n",
    "    FROM distinct_page_events\n",
    "    GROUP BY user_id, session_id\n",
    "),\n",
    "route_counts AS (\n",
    "    SELECT\n",
    "        route,\n",
    "        COUNT(*) AS count\n",
    "    FROM routes\n",
    "    GROUP BY route\n",
    ")\n",
    "SELECT\n",
    "    route,\n",
    "    count\n",
    "FROM route_counts\n",
    "ORDER BY count DESC\n",
    "LIMIT 30\n",
    "\"\"\")\n",
    "\n",
    "# Collect the results\n",
    "top_routes_list = result.collect()\n",
    "\n",
    "# Save the results to 'result.txt'\n",
    "with open('result.txt', 'w', encoding='utf-8') as f:\n",
    "    for row in top_routes_list:\n",
    "        f.write(f\"{row['route']}:{row['count']}\\n\")\n",
    "\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "\n",
    "sc = SparkContext(appName=\"UserRoutesRDD\")\n",
    "\n",
    "\n",
    "lines = sc.textFile(\"hdfs:/data/clickstream.csv\")\n",
    "\n",
    "# Remove the header\n",
    "header = lines.first()\n",
    "lines = lines.filter(lambda x: x != header)\n",
    "\n",
    "# Parse each line\n",
    "def parse_line(line):\n",
    "    import csv\n",
    "    from io import StringIO\n",
    "\n",
    "    # Use csv reader to handle quoted fields and separators\n",
    "    reader = csv.reader(StringIO(line), delimiter='\\t', quotechar='\"')\n",
    "    fields = next(reader)\n",
    "\n",
    "    # Parse fields\n",
    "    user_id = int(fields[0])\n",
    "    session_id = int(fields[1])\n",
    "    event_type = fields[2]\n",
    "    event_page = fields[3]\n",
    "    timestamp = int(fields[4])\n",
    "\n",
    "    return (user_id, session_id, event_type, event_page, timestamp)\n",
    "\n",
    "parsed_lines = lines.map(parse_line)\n",
    "\n",
    "# Key by (user_id, session_id)\n",
    "session_events = parsed_lines.map(lambda x: ((x[0], x[1]), x))\n",
    "\n",
    "# Group events by session\n",
    "session_grouped = session_events.groupByKey()\n",
    "\n",
    "# Process each session\n",
    "def process_session(events):\n",
    "    # Convert to list for multiple iterations\n",
    "    events_list = list(events)\n",
    "\n",
    "    # Sort events by timestamp\n",
    "    events_sorted = sorted(events_list, key=lambda x: x[4])\n",
    "\n",
    "    # Find the earliest error timestamp\n",
    "    min_error_timestamp = None\n",
    "    for event in events_sorted:\n",
    "        event_type = event[2]\n",
    "        timestamp = event[4]\n",
    "        if 'error' in event_type.lower():\n",
    "            min_error_timestamp = timestamp\n",
    "            break  # Earliest error timestamp found\n",
    "\n",
    "    # Process events up to and including the error timestamp\n",
    "    route_pages = []\n",
    "    last_page = None\n",
    "    for event in events_sorted:\n",
    "        event_type = event[2]\n",
    "        event_page = event[3]\n",
    "        timestamp = event[4]\n",
    "\n",
    "        if min_error_timestamp is not None and timestamp > min_error_timestamp:\n",
    "            break  # Stop processing events after error timestamp\n",
    "\n",
    "        if event_type == 'page':\n",
    "            # Remove consecutive duplicates\n",
    "            if event_page != last_page:\n",
    "                route_pages.append(event_page)\n",
    "                last_page = event_page\n",
    "\n",
    "    if route_pages:\n",
    "        route = '-'.join(route_pages)\n",
    "        return route\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "routes = session_grouped.mapValues(process_session)\n",
    "\n",
    "# Filter out sessions with empty or None routes\n",
    "routes_filtered = routes.filter(lambda x: x[1])\n",
    "\n",
    "# Count route frequencies\n",
    "route_counts = routes_filtered.map(lambda x: (x[1], 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Get top 30 routes\n",
    "route_counts_swapped = route_counts.map(lambda x: (x[1], x[0]))\n",
    "top_30 = route_counts_swapped.top(30)\n",
    "top_30_routes = sc.parallelize(top_30).map(lambda x: (x[1], x[0]))\n",
    "\n",
    "# Collect results\n",
    "top_routes_list = top_30_routes.collect()\n",
    "\n",
    "\n",
    "with open('result.txt', 'w', encoding='utf-8') as f:\n",
    "    for route, count in top_routes_list:\n",
    "        f.write(f\"{route}:{count}\\n\")\n",
    "\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, when, lower, lag, collect_list, concat_ws, min as spark_min\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"UserRoutesDF\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "df = spark.read.csv(\n",
    "    \"hdfs:/data/clickstream.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    sep='\\t',\n",
    "    quote='\"',\n",
    "    escape='\"'\n",
    ")\n",
    "\n",
    "# Identify error events\n",
    "df = df.withColumn('is_error', when(lower(col('event_type')).like('%error%'), 1).otherwise(0))\n",
    "\n",
    "# Get earliest error timestamp per session\n",
    "window_session = Window.partitionBy('user_id', 'session_id')\n",
    "\n",
    "df = df.withColumn('error_timestamp', when(col('is_error') == 1, col('timestamp')))\n",
    "df = df.withColumn('min_error_timestamp', spark_min('error_timestamp').over(window_session))\n",
    "\n",
    "# Include events up to and including the error timestamp\n",
    "df_filtered = df.filter(\n",
    "    (col('min_error_timestamp').isNull()) | (col('timestamp') <= col('min_error_timestamp'))\n",
    ")\n",
    "\n",
    "# Select only 'page' events\n",
    "df_pages = df_filtered.filter(col('event_type') == 'page')\n",
    "\n",
    "# Remove consecutive duplicates within sessions\n",
    "window_ordered = Window.partitionBy('user_id', 'session_id').orderBy('timestamp')\n",
    "df_pages = df_pages.withColumn('prev_event_page', lag('event_page').over(window_ordered))\n",
    "\n",
    "df_pages_distinct = df_pages.filter(\n",
    "    (col('event_page') != col('prev_event_page')) | col('prev_event_page').isNull()\n",
    ")\n",
    "\n",
    "# Collect routes for each session\n",
    "df_routes = df_pages_distinct.groupBy('user_id', 'session_id').agg(\n",
    "    collect_list('event_page').alias('pages_list')\n",
    ")\n",
    "\n",
    "# Create route strings\n",
    "df_routes = df_routes.withColumn('route', concat_ws('-', 'pages_list'))\n",
    "\n",
    "# Count route frequencies\n",
    "df_route_counts = df_routes.groupBy('route').count()\n",
    "\n",
    "# Get top 30 routes\n",
    "top_routes = df_route_counts.orderBy(col('count').desc()).limit(30)\n",
    "\n",
    "\n",
    "top_routes_list = top_routes.collect()\n",
    "\n",
    "# Save the results to 'result.txt'\n",
    "with open('result.txt', 'w', encoding='utf-8') as f:\n",
    "    for row in top_routes_list:\n",
    "        f.write(f\"{row['route']}:{row['count']}\\n\")\n",
    "\n",
    "\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "week-4-spark-homework"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
